{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b38867",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Currently finished up to Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import colab\n",
    "    !pip install --upgrade pip\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4560a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import urllib\n",
    "\n",
    "import absl\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "tf.get_logger().propagate = False\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "%load_ext tfx.orchestration.experimental.interactive.notebook_extensions.skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af94d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec611a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the root directory for your TFX pip package installation.\n",
    "_tfx_root = tfx.__path__[0]\n",
    "\n",
    "# This is the directory containing the TFX Loan Pipeline.\n",
    "_loan_root = os.path.join(_tfx_root, 'examples/loan_data_pipeline')\n",
    "\n",
    "# This is the path where your model will be pushed for serving.\n",
    "_serving_model_dir = os.path.join(\n",
    "    tempfile.mkdtemp(), 'serving_model/loan_simple')\n",
    "\n",
    "# Set up logging.\n",
    "absl.logging.set_verbosity(absl.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
    "DATA_PATH = 'https://raw.githubusercontent.com/Jeromeschmidt/LendingClubLoanData/main/data/1.csv'\n",
    "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
    "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head {_data_filepath}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145f1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = InteractiveContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb7a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gen = tfx.components.CsvExampleGen(input_base=_data_root)\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf74f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = example_gen.outputs['examples'].get()[0]\n",
    "print(artifact.split_names, artifact.uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d79962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the training examples, which is a directory\n",
    "loan_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'Split-train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(loan_uri, name)\n",
    "                      for name in os.listdir(loan_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "# # Iterate over the first 3 records and decode them.\n",
    "# for tfrecord in dataset.take(3):\n",
    "#   serialized_example = tfrecord.numpy()\n",
    "#   example = tf.train.Example()\n",
    "#   example.ParseFromString(serialized_example)\n",
    "#   pp.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b56a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics_gen = tfx.components.StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a392b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_gen = tfx.components.SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=False)\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3095e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_validator = tfx.components.ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4564b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loan_constants_module_file = 'loan_constants.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7475ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_loan_constants_module_file}\n",
    "\n",
    "COLUMN_FLOAT = ['loan_amnt',\n",
    " 'funded_amnt',\n",
    " 'funded_amnt_inv',\n",
    " 'int_rate',\n",
    " 'installment',\n",
    " 'annual_inc',\n",
    " 'dti',\n",
    " 'delinq_2yrs',\n",
    " 'fico_range_low',\n",
    " 'fico_range_high',\n",
    " 'inq_last_6mths',\n",
    " 'open_acc',\n",
    " 'pub_rec',\n",
    " 'revol_bal',\n",
    " 'revol_util',\n",
    " 'total_acc',\n",
    " 'out_prncp',\n",
    " 'out_prncp_inv',\n",
    " 'total_pymnt',\n",
    " 'total_pymnt_inv',\n",
    " 'total_rec_prncp',\n",
    " 'total_rec_int',\n",
    " 'total_rec_late_fee',\n",
    " 'recoveries',\n",
    " 'collection_recovery_fee',\n",
    " 'last_pymnt_amnt',\n",
    " 'last_fico_range_high',\n",
    " 'last_fico_range_low',\n",
    " 'collections_12_mths_ex_med',\n",
    " 'policy_code',\n",
    " 'acc_now_delinq',\n",
    " 'tot_coll_amt',\n",
    " 'tot_cur_bal',\n",
    " 'total_rev_hi_lim',\n",
    " 'acc_open_past_24mths',\n",
    " 'avg_cur_bal',\n",
    " 'bc_open_to_buy',\n",
    " 'bc_util',\n",
    " 'chargeoff_within_12_mths',\n",
    " 'delinq_amnt',\n",
    " 'mo_sin_old_il_acct',\n",
    " 'mo_sin_old_rev_tl_op',\n",
    " 'mo_sin_rcnt_rev_tl_op',\n",
    " 'mo_sin_rcnt_tl',\n",
    " 'mort_acc',\n",
    " 'mths_since_recent_bc',\n",
    " 'mths_since_recent_inq',\n",
    " 'num_accts_ever_120_pd',\n",
    " 'num_actv_bc_tl',\n",
    " 'num_actv_rev_tl',\n",
    " 'num_bc_sats',\n",
    " 'num_bc_tl',\n",
    " 'num_il_tl',\n",
    " 'num_op_rev_tl',\n",
    " 'num_rev_accts',\n",
    " 'num_rev_tl_bal_gt_0',\n",
    " 'num_sats',\n",
    " 'num_tl_120dpd_2m',\n",
    " 'num_tl_30dpd',\n",
    " 'num_tl_90g_dpd_24m',\n",
    " 'num_tl_op_past_12m',\n",
    " 'pct_tl_nvr_dlq',\n",
    " 'percent_bc_gt_75',\n",
    " 'pub_rec_bankruptcies',\n",
    " 'tax_liens',\n",
    " 'tot_hi_cred_lim',\n",
    " 'total_bal_ex_mort',\n",
    " 'total_bc_limit',\n",
    " 'total_il_high_credit_limit']\n",
    "\n",
    "COLUMN_OBJECT = ['term',\n",
    " 'emp_length',\n",
    " 'home_ownership',\n",
    " 'verification_status',\n",
    " 'issue_d',\n",
    " 'loan_status',\n",
    " 'pymnt_plan',\n",
    " 'purpose',\n",
    " 'zip_code',\n",
    " 'addr_state',\n",
    " 'earliest_cr_line',\n",
    " 'initial_list_status',\n",
    " 'last_pymnt_d',\n",
    " 'last_credit_pull_d',\n",
    " 'application_type',\n",
    " 'hardship_flag',\n",
    " 'disbursement_method',\n",
    " 'debt_settlement_flag']\n",
    "\n",
    "LABEL_KEY = 'grade'\n",
    "\n",
    "BUCKET_SIZE = 100\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key + '_xf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da69bad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loan_transform_module_file = 'loan_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb365cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_loan_transform_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import loan_constants\n",
    "\n",
    "_transformed_name = loan_constants.transformed_name\n",
    "_FLOAT_FEATURE_KEYS = loan_constants.COLUMN_FLOAT\n",
    "_CATEGORICAL_FEATURE_KEYS = loan_constants.COLUMN_OBJECT\n",
    "_LABEL_KEY = loan_constants.LABEL_KEY\n",
    "_BUCKET_SIZE = loan_constants.BUCKET_SIZE\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "    Args:\n",
    "    inputs: map from feature keys to raw not-yet-transformed features.\n",
    "    Returns:\n",
    "    Map from string feature key to transformed feature operations.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "    for key in _FLOAT_FEATURE_KEYS:\n",
    "        # Preserve this feature as a dense float, setting nan's to the mean.\n",
    "        outputs[_transformed_name(key)] = tft.scale_to_z_score(_fill_in_missing(inputs[key]))\n",
    "\n",
    "    for key in _CATEGORICAL_FEATURE_KEYS:\n",
    "        # Build a vocabulary for this feature.\n",
    "        outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(_fill_in_missing(inputs[key]), _BUCKET_SIZE)\n",
    "\n",
    "    outputs[_transformed_name(_LABEL_KEY)] = tft.compute_and_apply_vocabulary(_fill_in_missing(inputs[_LABEL_KEY]), 7)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def _fill_in_missing(x):\n",
    "    \"\"\"Replace missing values in a SparseTensor.\n",
    "    Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
    "    Args:\n",
    "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
    "      in the second dimension.\n",
    "    Returns:\n",
    "    A rank 1 tensor where missing values of `x` have been filled in.\n",
    "    \"\"\"\n",
    "    if not isinstance(x, tf.sparse.SparseTensor):\n",
    "        return x\n",
    "    \n",
    "    default_value = '' if x.dtype == tf.string else 0\n",
    "    return tf.squeeze(\n",
    "      tf.sparse.to_dense(\n",
    "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "          default_value),\n",
    "      axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87385086",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = tfx.components.Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=os.path.abspath(_loan_transform_module_file))\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a695d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
    "os.listdir(train_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f051fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the transformed examples, which is a directory\n",
    "train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "# Iterate over the first 3 records and decode them.\n",
    "# for tfrecord in dataset.take(10):\n",
    "#     serialized_example = tfrecord.numpy()\n",
    "#     example = tf.train.Example()\n",
    "#     example.ParseFromString(serialized_example)\n",
    "#     pp.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb04759",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loan_trainer_module_file = 'loan_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_loan_trainer_module_file}\n",
    "\n",
    "from typing import List, Text\n",
    "\n",
    "import os\n",
    "import absl\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "import loan_constants\n",
    "\n",
    "_FLOAT_FEATURE_KEYS = loan_constants.COLUMN_FLOAT\n",
    "_CATEGORICAL_FEATURE_KEYS = loan_constants.COLUMN_OBJECT\n",
    "_transformed_name = loan_constants.transformed_name\n",
    "_LABEL_KEY = loan_constants.LABEL_KEY\n",
    "_BUCKET_SIZE = loan_constants.BUCKET_SIZE\n",
    "\n",
    "\n",
    "def _transformed_names(keys):\n",
    "    return [_transformed_name(key) for key in keys]\n",
    "\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "    \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(_LABEL_KEY)\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        return model(transformed_features)\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "    Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "    Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)),\n",
    "      tf_transform_output.transformed_metadata.schema)\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "    \"\"\"Creates a DNN Keras model for classifying Loan data.\n",
    "\n",
    "    Returns:\n",
    "    A keras Model.\n",
    "    \"\"\"\n",
    "    real_valued_columns = [\n",
    "      tf.feature_column.numeric_column(key, shape=())\n",
    "      for key in _transformed_names(_FLOAT_FEATURE_KEYS)\n",
    "    ]\n",
    "    categorical_columns = [\n",
    "      tf.feature_column.categorical_column_with_identity(\n",
    "          key, num_buckets=_BUCKET_SIZE, default_value=0)\n",
    "      for key in _transformed_names(_CATEGORICAL_FEATURE_KEYS)\n",
    "    ]\n",
    "\n",
    "\n",
    "    indicator_column = [\n",
    "      tf.feature_column.indicator_column(categorical_column)\n",
    "      for categorical_column in categorical_columns\n",
    "    ]\n",
    "    \n",
    "\n",
    "    model = _wide_and_deep_classifier(\n",
    "      wide_columns=indicator_column,\n",
    "      deep_columns=real_valued_columns)\n",
    "    return model\n",
    "\n",
    "\n",
    "def _wide_and_deep_classifier(wide_columns, deep_columns):\n",
    "    \"\"\"Build a simple keras wide and deep model.\n",
    "\n",
    "    Args:\n",
    "    wide_columns: Feature columns wrapped in indicator_column for wide (linear)\n",
    "      part of the model.\n",
    "    deep_columns: Feature columns for deep part of the model.\n",
    "    dnn_hidden_units: [int], the layer sizes of the hidden DNN.\n",
    "\n",
    "    Returns:\n",
    "    A Wide and Deep Keras model\n",
    "    \"\"\"\n",
    "\n",
    "    # Keras needs the feature definitions at compile time.\n",
    "    input_layers = {\n",
    "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype=tf.float32)\n",
    "      for colname in _transformed_names(_FLOAT_FEATURE_KEYS)\n",
    "    }\n",
    "    input_layers.update({\n",
    "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
    "      for colname in _transformed_names(_CATEGORICAL_FEATURE_KEYS)\n",
    "    })\n",
    "\n",
    "    deep = tf.keras.layers.DenseFeatures(deep_columns)(input_layers)\n",
    "    deep = tf.keras.layers.Dense(128, activation=\"relu\")(deep)\n",
    "    deep = tf.keras.layers.Dense(64, activation=\"relu\")(deep)\n",
    "    wide = tf.keras.layers.DenseFeatures(wide_columns)(input_layers)\n",
    "\n",
    "    output = tf.keras.layers.Dense(7, activation=\"softmax\")(tf.keras.layers.concatenate([deep, wide]))\n",
    "\n",
    "    model = tf.keras.Model(input_layers, output)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "    \"\"\"Train the model based on given args.\n",
    "\n",
    "    Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "    train_dataset = _input_fn(fn_args.train_files, fn_args.data_accessor, \n",
    "                            tf_transform_output, 32)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, fn_args.data_accessor, \n",
    "                           tf_transform_output, 32)\n",
    "\n",
    "    model = _build_keras_model()\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=fn_args.model_run_dir, update_freq='batch')\n",
    "    model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps,\n",
    "      callbacks=[tensorboard_callback])\n",
    "\n",
    "    signatures = {\n",
    "      'serving_default':\n",
    "          _get_serve_tf_examples_fn(model,\n",
    "                                    tf_transform_output).get_concrete_function(\n",
    "                                        tf.TensorSpec(\n",
    "                                            shape=[None],\n",
    "                                            dtype=tf.string,\n",
    "                                            name='examples')),\n",
    "    }\n",
    "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a12f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tfx.components.Trainer(\n",
    "    module_file=os.path.abspath(_loan_trainer_module_file),\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    train_args=tfx.proto.TrainArgs(num_steps=10000),\n",
    "    eval_args=tfx.proto.EvalArgs(num_steps=5000))\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_artifact_dir = trainer.outputs['model'].get()[0].uri\n",
    "pp.pprint(os.listdir(model_artifact_dir))\n",
    "model_dir = os.path.join(model_artifact_dir, 'Format-Serving')\n",
    "pp.pprint(os.listdir(model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aaf066",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run_artifact_dir = trainer.outputs['model_run'].get()[0].uri\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {model_run_artifact_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c2dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config = tfma.EvalConfig(\n",
    "    model_specs=[\n",
    "        # This assumes a serving model with signature 'serving_default'. If\n",
    "        # using estimator based EvalSavedModel, add signature_name: 'eval' and \n",
    "        # remove the label_key.\n",
    "        tfma.ModelSpec(label_key='grade')\n",
    "    ],\n",
    "    metrics_specs=[\n",
    "        tfma.MetricsSpec(\n",
    "            # The metrics added here are in addition to those saved with the\n",
    "            # model (assuming either a keras model or EvalSavedModel is used).\n",
    "            # Any metrics added into the saved model (for example using\n",
    "            # model.compile(..., metrics=[...]), etc) will be computed\n",
    "            # automatically.\n",
    "            # To add validation thresholds for metrics saved with the model,\n",
    "            # add them keyed by metric name to the thresholds map.\n",
    "            metrics=[\n",
    "                tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                tfma.MetricConfig(class_name='Accuracy',\n",
    "                  threshold=tfma.MetricThreshold(\n",
    "                      value_threshold=tfma.GenericValueThreshold(\n",
    "                          lower_bound={'value': 0.9}),\n",
    "                      # Change threshold will be ignored if there is no\n",
    "                      # baseline model resolved from MLMD (first run).\n",
    "                      change_threshold=tfma.GenericChangeThreshold(\n",
    "                          direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                          absolute={'value': -1e-10})))\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    "    slicing_specs=[\n",
    "        # An empty slice spec means the overall slice, i.e. the whole dataset.\n",
    "        tfma.SlicingSpec(),\n",
    "        # Data can be sliced along a feature column. In this case, data is\n",
    "        # sliced along feature column trip_start_hour.\n",
    "        tfma.SlicingSpec(feature_keys=['loan_amnt'])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3187ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TFMA to compute a evaluation statistics over features of a model and\n",
    "# validate them against a baseline.\n",
    "\n",
    "# The model resolver is only required if performing model validation in addition\n",
    "# to evaluation. In this case we validate against the latest blessed model. If\n",
    "# no model has been blessed before (as in this case) the evaluator will make our\n",
    "# candidate the first blessed model.\n",
    "model_resolver = tfx.dsl.Resolver(\n",
    "      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
    "      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "      model_blessing=tfx.dsl.Channel(\n",
    "          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n",
    "              'latest_blessed_model_resolver')\n",
    "context.run(model_resolver)\n",
    "\n",
    "evaluator = tfx.components.Evaluator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'],\n",
    "    baseline_model=model_resolver.outputs['model'],\n",
    "    eval_config=eval_config)\n",
    "context.run(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f6cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918dd66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.show(evaluator.outputs['evaluation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fadf0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "# Get the TFMA output result path and load the result.\n",
    "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\n",
    "tfma_result = tfma.load_eval_result(PATH_TO_RESULT)\n",
    "\n",
    "# Show data sliced along feature column loan_amnt.\n",
    "tfma.view.render_slicing_metrics(\n",
    "    tfma_result, slicing_column='loan_amnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ada8039",
   "metadata": {},
   "outputs": [],
   "source": [
    "blessing_uri = evaluator.outputs['blessing'].get()[0].uri\n",
    "!ls -l {blessing_uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_RESULT = evaluator.outputs['evaluation'].get()[0].uri\n",
    "print(tfma.load_validation_result(PATH_TO_RESULT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher = tfx.components.Pusher(\n",
    "    model=trainer.outputs['model'],\n",
    "    model_blessing=evaluator.outputs['blessing'],\n",
    "    push_destination=tfx.proto.PushDestination(\n",
    "        filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "            base_directory=_serving_model_dir)))\n",
    "context.run(pusher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0617733",
   "metadata": {},
   "outputs": [],
   "source": [
    "pusher.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_uri = pusher.outputs['pushed_model'].get()[0].uri\n",
    "model = tf.saved_model.load(push_uri)\n",
    "\n",
    "for item in model.signatures.items():\n",
    "    pp.pprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc25a6f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
